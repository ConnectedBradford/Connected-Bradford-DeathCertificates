{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8f255-5050-4314-805d-d1079a145d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Notes \n",
    "#Open a terminal and run: conda install -c conda-forge scipy\n",
    "#then conda install -c conda-forge tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5445ac0-c36e-4c4a-aa5c-c8597e42cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_num_description_column(col_name, table_name):\n",
    "    \"\"\"\n",
    "    Takes a column name and a table name, returning a string with descriptive \n",
    "    statistics for the column specified.\n",
    "\n",
    "    Calculates the mean, median, max, min and IQR for the specified column using \n",
    "    BigQuery SQL and returns a string with the results concatenated together.\n",
    "\n",
    "    Args:\n",
    "        col_name (str): The name of a the numeric column.\n",
    "        table_name (str): The name of the table containing the specified column.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with the mean, median, max, min and IQR for the specified column.\n",
    "    \"\"\"\n",
    "    sql_query = f\"\"\"\n",
    "        WITH stats AS (\n",
    "            SELECT\n",
    "                AVG({col_name}) AS mean,\n",
    "                MAX({col_name}) AS max,\n",
    "                MIN({col_name}) AS min,\n",
    "                APPROX_QUANTILES({col_name}, 2)[OFFSET(1)] AS median,\n",
    "                APPROX_QUANTILES({col_name}, 4)[OFFSET(1)] AS q1,\n",
    "                APPROX_QUANTILES({col_name}, 4)[OFFSET(3)] AS q3\n",
    "            FROM `{table_name}`\n",
    "        )\n",
    "        SELECT CONCAT('Mean: ', CAST(mean AS STRING),  \n",
    "                      ', Median: ', CAST(median AS STRING), \n",
    "                      ', Max: ', CAST(max AS STRING), \n",
    "                      ', Min: ', CAST(min AS STRING),  \n",
    "                      ', IQR: ', CAST(q3 - q1 AS STRING))\n",
    "        FROM stats\n",
    "    \"\"\"\n",
    "    return pd.read_gbq(sql_query).iloc[0,0]\n",
    "\n",
    "\n",
    "def get_date_description_column(col_name, table_name):\n",
    "    \"\"\"\n",
    "    Takes a column name and a table name, returning a string with the min and max \n",
    "    dates.\n",
    "\n",
    "    Calculates the min and max dates for the specified column using BigQuery SQL \n",
    "    and returns a string with the results concatenated together.\n",
    "\n",
    "    Args:\n",
    "        col_name (str): The name of a date column.\n",
    "        table_name (str): The name of the table containing the specified column.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with the min and max dates for the specified column.\n",
    "    \"\"\"\n",
    "    sql_query = f\"\"\"\n",
    "        WITH stats AS (\n",
    "            SELECT \n",
    "                MAX({col_name}) AS max_date, \n",
    "                MIN({col_name}) AS min_date \n",
    "            FROM `{table_name}`\n",
    "        )\n",
    "        SELECT\n",
    "            CONCAT('From: ', CAST(min_date AS STRING), \n",
    "                   ' To: ', CAST(max_date AS STRING))\n",
    "        FROM stats\n",
    "    \"\"\"\n",
    "    return pd.read_gbq(sql_query).iloc[0,0]\n",
    "\n",
    "\n",
    "def get_bool_description_column(col_name, table_name):\n",
    "    \"\"\"\n",
    "    Takes a column name and a table name, returning a the count of `True` and \n",
    "    `False` values.\n",
    "\n",
    "    Calculates the count of `True` and `False` values for the specified column \n",
    "    using BigQuery SQL and returns a string with the results concatenated \n",
    "    together.\n",
    "\n",
    "    Args:\n",
    "        col_name (str): The name of the boolean column.\n",
    "        table_name (str): The name of the table containing the specified column.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with the count of `True` and `False` values for the \n",
    "             specified column.\n",
    "    \"\"\"\n",
    "    sql_query = f\"\"\"\n",
    "        WITH stats AS (\n",
    "            SELECT\n",
    "                COUNTIF({col_name} = TRUE) AS true_count,\n",
    "                COUNTIF({col_name} = FALSE) AS false_count\n",
    "            FROM `{table_name}`\n",
    "        )\n",
    "        SELECT\n",
    "            CONCAT('False: ', CAST(false_count AS STRING), \n",
    "                   ', True: ', CAST(true_count AS STRING))\n",
    "        FROM stats\n",
    "    \"\"\"\n",
    "    return pd.read_gbq(sql_query).iloc[0,0]\n",
    "\n",
    "\n",
    "def get_string_description_column(col_name, table_name):\n",
    "    sql_query = f\"\"\"\n",
    "        WITH top_entries AS (\n",
    "            SELECT {col_name}, COUNT(*) AS count\n",
    "            FROM `{table_name}`\n",
    "            GROUP BY {col_name}\n",
    "            ORDER BY count DESC\n",
    "            LIMIT 5 \n",
    "        ),\n",
    "        total_entries AS (\n",
    "            SELECT COUNT(DISTINCT {col_name}) AS total_count\n",
    "            FROM `{table_name}` \n",
    "        )\n",
    "        SELECT IF((SELECT total_count FROM total_entries) > 5, \n",
    "                   CONCAT('Top 5: ', STRING_AGG(CONCAT({col_name}, ': ', \n",
    "                          CAST(count AS STRING)), ', ')), \n",
    "                   STRING_AGG(CONCAT({col_name}, ': ', \n",
    "                              CAST(count AS STRING)), ', '))\n",
    "        FROM top_entries\n",
    "    \"\"\"\n",
    "    return pd.read_gbq(sql_query).iloc[0,0]\n",
    "\n",
    "\n",
    "def create_data_dict(dataset_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a data dictionary table for a BigQuery dataset.\n",
    "    \n",
    "    Takes the ID of a BigQuery dataset and creates a data_dict table in the same \n",
    "    dataset. `data_dict` contains information about tables in the \n",
    "    dataset with names prefixed \" tbl_\" or \"cb_\":\n",
    "    table name, column name, data type, and a summary \n",
    "    description of each column. `description` column includes summary statistics \n",
    "    for numeric columns (mean, median, IQR, min,  max), the number of unique \n",
    "    values and top 5 values for string columns, the  date range for date \n",
    "    columns, and the count of True and False values for boolean columns. \n",
    "    \n",
    "    Args:\n",
    "        dataset_id (str): The ID of the BigQuery dataset.\n",
    "        \n",
    "    Output:\n",
    "        None - `data_dict` table is uploaded to biqquery dataset at \"dataset_id\"\n",
    "    \"\"\"\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    tables = list(client.list_tables(dataset_ref))\n",
    "    rows = []\n",
    "    table_count = 0\n",
    "    output_dict = {\n",
    "        \"table_name\": [],\n",
    "        \"column_name\": [],\n",
    "        \"data_type\": [],\n",
    "        \"description\": []\n",
    "    }\n",
    "    for table in tables:\n",
    "        if table.table_id.startswith(\"tbl_\") or table.table_id.startswith(\"cb_\"):\n",
    "            table_count += 1\n",
    "            print(f\"Processing table {table_count} of {len(tables)}: {table.table_id}\")\n",
    "            table_ref = dataset_ref.table(table.table_id)\n",
    "            table = client.get_table(table_ref)\n",
    "            for schema_field in tqdm(table.schema):\n",
    "                output_dict[\"table_name\"].append(table.table_id)\n",
    "                output_dict[\"column_name\"].append(schema_field.name)\n",
    "                output_dict[\"data_type\"].append(schema_field.field_type)\n",
    "                full_table_id = f\"{dataset_id}.{table.table_id}\"\n",
    "                if schema_field.field_type == \"STRING\":\n",
    "                    output_dict[\"description\"].append(\n",
    "                        get_string_description_column(schema_field.name, \n",
    "                                                      full_table_id) \n",
    "                    )\n",
    "                elif schema_field.field_type in [\"INTEGER\", \"FLOAT\", \"NUMERIC\", \"INT64\", \"FLOAT64\"]:\n",
    "                    output_dict[\"description\"].append(\n",
    "                        get_num_description_column(schema_field.name, \n",
    "                                                      full_table_id) \n",
    "                    )\n",
    "                elif schema_field.field_type in [\"DATE\", \"TIMESTAMP\", \"DATETIME\",\"TIME\"]:\n",
    "                    output_dict[\"description\"].append(\n",
    "                        get_date_description_column(schema_field.name, \n",
    "                                                      full_table_id) \n",
    "                    )\n",
    "                elif schema_field.field_type in [\"BOOL\", \"BOOLEAN\"]:\n",
    "                    output_dict[\"description\"].append(\n",
    "                        get_bool_description_column(schema_field.name, \n",
    "                                                      full_table_id) \n",
    "                    )\n",
    "    output_df = pd.DataFrame(output_dict)\n",
    "\n",
    "    \n",
    "    output_df.to_gbq(f\"{dataset_id}.data_dictionary\", progress_bar=False)\n",
    "    print(\"Finished creating data_dict table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71c9c81-a37a-4acb-aa55-59f595389957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/conda/lib/python3.7/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table 1 of 6: tbl_NEC_Deaths_QWO_20230817_RELEASE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pandas-gbq'. pandas-gbq is required to load data from Google BigQuery. See the docs: https://pandas-gbq.readthedocs.io. Use pip or conda to install pandas-gbq.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_14221/3226271783.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_data_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CB_FDM_DeathCertificates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/ipykernel_14221/1567191356.py\u001b[0m in \u001b[0;36mcreate_data_dict\u001b[0;34m(dataset_id)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     output_dict[\"description\"].append(\n\u001b[1;32m    172\u001b[0m                         get_string_description_column(schema_field.name, \n\u001b[0;32m--> 173\u001b[0;31m                                                       full_table_id) \n\u001b[0m\u001b[1;32m    174\u001b[0m                     )\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mschema_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"INTEGER\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FLOAT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NUMERIC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"INT64\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FLOAT64\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_14221/1567191356.py\u001b[0m in \u001b[0;36mget_string_description_column\u001b[0;34m(col_name, table_name)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mFROM\u001b[0m \u001b[0mtop_entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_gbq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, progress_bar_type)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gbq\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mWrite\u001b[0m \u001b[0ma\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0mto\u001b[0m \u001b[0mGoogle\u001b[0m \u001b[0mBigQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \"\"\"\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mpandas_gbq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36m_try_import\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;34m\"See the docs: https://pandas-gbq.readthedocs.io.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpandas_gbq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas_gbq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpandas_gbq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/compat/_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pandas-gbq'. pandas-gbq is required to load data from Google BigQuery. See the docs: https://pandas-gbq.readthedocs.io. Use pip or conda to install pandas-gbq."
     ]
    }
   ],
   "source": [
    "create_data_dict(\"CB_FDM_DeathCertificates\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-2.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-2:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
